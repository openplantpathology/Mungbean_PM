
```{r MEsummary_Libraries2, message=FALSE, include=FALSE}
if (!require("pacman"))
   install.packages("pacman")
pacman::p_load(tidyverse, kableExtra, bomrang, lme4, RColorBrewer,metafor, netmeta)

if (!require("theme.usq"))
   devtools::install_github("adamhsparks/theme.usq")
library(theme.usq)
theme_set(theme_usq())

# Data
slimmer_PM_dat <- read.csv("cache/slimmer_PM_clusterdat.csv")
```

# Meta-analysis
## Grain yield meta-analysis

Let's get started with the analysis by first finding the best model fit that answers our research question.  

> Which spray management scenario provides the greatest yield protection from powdery mildew.  

   - Grain yield is our response variable and will be converted to Kg / Ha.  
   - Trial, which resolves combinations of categorical variables: year, location, row spacing, fungicide dose and cultivar; is set as a random intercept.  
   - We will test spray management (our treatment) as a fixed effect and random slope to trial.  

First lets compare models with and without a log transformed response variable to evaluate if the log transformed model better fits our assumptions of normality.  
Next we will test whether including `spray_management` as a fixed effect variable significantly accounts for enough variation to be included in the model.  
Finally we will test whether including `spray_management`  as a random slope significantly accounts for enough variation to be used in the model.  


```{r LME_slimmer_cluster_disease_mod}
m8 <- lmer(grain_yield.t.ha * 1000 ~ factor(spray_management) +
              (factor(spray_management) | trial),
           data = slimmer_PM_dat)

m9 <- lmer(log(grain_yield.t.ha * 1000) ~ factor(spray_management) +
              (factor(spray_management) | trial),
           data = slimmer_PM_dat)

m10 <- lmer(log(grain_yield.t.ha * 1000) ~
               (factor(spray_management) | trial),
            data = slimmer_PM_dat)

m11 <-
   lmer(log(grain_yield.t.ha * 1000) ~ factor(spray_management) +
           (1 | trial),
        data = slimmer_PM_dat)

```

```{r m8_vs_m9}
anova(m8, m9) # m9 is significantly better model
```
There is a significant difference between `m8` and `m9`, with `m9` showing the best fit and lowest AIC: `r min(anova(m8, m9)$AIC)`.  

```{r m9_vs_m10}
anova(m9, m10) # m9 is significantly better model
```
There is a significant difference between `m10` and `m9`, with `m9` showing the best fit and lowest AIC: `r min(anova(m9, m10)$AIC)`.  


```{r m9_vs_m11}
anova(m9, m11) # m11 is a simpler model which is no different to m9
```

There is no difference between the models, which means the the simpler model `m11` is the better model, also making it the best model from the four above with the lower AIC of -`r min(anova(m9, m11)$AIC)` 

```{r}
summary(m11)
```

This linear mixed effect model shows indicates:  

   - A single `early` spray before first sign of powdery mildew is not likely to increase yields.  
   
   - One spray at the `recommended` timing, or two sprays starting at the `recommended` first spray, are likely to produce significantly higher grain yields compared to the no spray control.  
   
   - The `Late_plus` spray which has one or more follow-up sprays after first sign are likely to increase the mean grain yield.  
   
   - `Recommended_plus` spray treatments showed the highest mean grain yield, and was significantly higher than the no spray control. However showed no difference to either the `recommended` or `Late_plus` treatments.  


#### Imputing sample variances from Mean squares
```{r imputing_MSE_dataSpread, eval=FALSE, include=FALSE}
slimmer_PM_dat <- read.csv("cache/slimmer_PM_clusterdat.csv")
hist_usq(unique(slimmer_PM_dat$Y_Msquare))
hist_usq(log(unique(slimmer_PM_dat$Y_Msquare))) # log mean square has a more normal distribution

```

Imputing using a log transformation of the data is required. Which trials need variance imputation?  

```{r imputing_MSE, eval=FALSE, include=FALSE}
TrialMSQ <- slimmer_PM_dat %>%
   group_by(trial_ref,location,year)%>%
   summarise(unique(Y_Msquare))

TrialMSQ[is.na(TrialMSQ$`unique(Y_Msquare)`), c("trial_ref", "location","year")]
```



```{r imputation_MSE, eval=FALSE, include=FALSE}
for (i in TrialMSQ[is.na(TrialMSQ$`unique(Y_Msquare)`), ]$trial_ref) {
   slimmer_PM_dat[slimmer_PM_dat$trial_ref == i, "Y_Msquare"] <-
      exp(rnorm(
         n = 1,
         mean = mean(log(TrialMSQ$`unique(Y_Msquare)`), na.rm = TRUE),
         sd(log(TrialMSQ$`unique(Y_Msquare)`), na.rm = TRUE)
      ))
}
```

The use of these imputed MSE was giving a varied outcome each time it was run. 
The next code chunk removes the studies without variation to observe what the meta-analysis outcome would be.

```{r remove_NA_MSE}
slimmer_PM_dat <- 
   slimmer_PM_dat[!is.na(slimmer_PM_dat$Y_Msquare),]

```


Before analysis let's have a look at the trimmed down modified data.  

First, how well the data is compared across the trial years and trials.  

```{r table_of_nTreatments}
kable(table(slimmer_PM_dat$spray_management, slimmer_PM_dat$year),
                  align = rep('c', 8),
      caption = "Which treatments and how many treatments are represented in each year") %>%  
   kable_styling(
      "striped",
      fixed_thead = TRUE,
      full_width = FALSE,
      position = "center"
   )
```
Treatments Late_plus and early don't have very good comparison to other treatments.  
  

```{r treatment_means_plot}
# class spray_management as a factor and reorder them for the plot
slimmer_PM_dat$spray_management <-
   factor(slimmer_PM_dat$spray_management,
          levels(factor(slimmer_PM_dat$spray_management))[rev(c(1, 2, 5, 6, 3,4))])

slimmer_PM_dat %>%
   ggplot(aes(y = grain_yield.t.ha, x = spray_management)) +
   geom_boxplot() +
   #geom_point(position = "jitter", alpha = 1/5)+
   geom_jitter(width = 0.1, alpha = 1 / 5) +
   labs(x = "Spray management variable",
        y = "Grain yield (t/Ha)",
        title = "Mean grain yield from each treatment \n categorised by spray management scenario") +
   theme(plot.title = element_text(hjust = 0.5)) +
   geom_hline(yintercept = 0, linetype = 2) +
   coord_flip()
```

There seems like little difference between the treatments, with exception to Late_plus. 
Let's do this plot again, but lets look at the proportional mean difference between the treatments and the no spray control for each study.  
This should reduce variation in yield due between trials.  

```{r plot_yieldProportion}
slimmer_PM_dat$spray_management <-
   factor(slimmer_PM_dat$spray_management, rev(
      c(
         "control",
         "Early",
         "Recommended",
         "Recommended_plus",
         "Late",
         "Late_plus"
      )
   ))

slimmer_PM_dat %>%
   ggplot(aes(y = prop_yield_gain, x = spray_management)) +
   geom_boxplot() +
   geom_jitter(width = 0.1, alpha = 1 / 5) +
   labs(x = "Spray management variable",
        y = "Grain yield (t/Ha)",
        title = "Mean grain yield difference to the control for each treatment \n categorised by spray management scenario") +
   theme(plot.title = element_text(hjust = 0.5)) +
   geom_hline(yintercept = 0, linetype = 2) +
   coord_flip()
```

Using the mean difference in the treatment effect seems to show the impact of each treatment better than just raw yield.
We know there is a good deal of variability in our studies and between them. 
Mungbean produces variable yields between seasons so we should use a response that highlights the difference in the treatment effects that we are interested in and reduce the variability.
Lets calculate the standardised mean difference for each treatment to reduce the variability in the meta-analysis.

### Caluculate standardised mean differences
We may wish to undertake the meta-analysis using standardised mean differences. 
Lets calculate them and add them to the dataframe.

#### Calculate sample variance from mean square errors

```{r MSE_2_SVar}
slimmer_PM_dat$vi <-
   slimmer_PM_dat$Y_Msquare / (slimmer_PM_dat$n * slimmer_PM_dat$grain_yield.t.ha ^
                                  2)
```


```{r}
slimmer_PM_dat$pooledSD <- NA
slimmer_PM_dat$vi_C <- NA
for(T_ref in slimmer_PM_dat$trial_ref){
   
   # First we neee to Calculate the pooled standard deviation
   slimmer_PM_dat[slimmer_PM_dat$trial_ref == T_ref, "pooledSD"] <- 
   sqrt(sum((slimmer_PM_dat[slimmer_PM_dat$trial_ref == T_ref,"n" ]-1) * slimmer_PM_dat[slimmer_PM_dat$trial_ref == T_ref,"vi"])/
           (sum( slimmer_PM_dat[slimmer_PM_dat$trial_ref == T_ref,"n"]) - nrow( slimmer_PM_dat[slimmer_PM_dat$trial_ref == T_ref, ])))
 
   # Then Create a column with the value of the mean grain yield of the no spray control for comparisons to the treatments means
   slimmer_PM_dat[slimmer_PM_dat$trial_ref == T_ref,"yi_C"] <-  
      mean(slimmer_PM_dat[slimmer_PM_dat$trial_ref == T_ref &
                        slimmer_PM_dat$fungicide_ai == "control",
                        "grain_yield.t.ha"], na.rm = TRUE)

}
# Calculate standardised mean difference
slimmer_PM_dat$grain_SMD <- (slimmer_PM_dat$grain_yield.t.ha - slimmer_PM_dat$yi_C)/ slimmer_PM_dat$pooledSD 

```



## Meta-analysis  
### metafor package  
Let's load the `metafor` package we are using to analyse the data, then rearrange the factors we want to examine by placing the control treatment first. This way all treatments will be compared to the no spray controls.  
Next we are log transforming the grain yield and calculating the variance from the trial mean squares.  
Finally we will assign factor classes to the main variables in the meta-analysis. Note that variable trial is a combination of:

   * Trial identifier  

   * Trial year  

   * Trial location  

   * Host genotype  

   * Trial row spacing  

```{r metafor_organisation}
#remove controls from the data
dat1 <- slimmer_PM_dat[!slimmer_PM_dat$fungicide_ai == "control",]

dat1$spray_management <-
   factor(
      dat1$spray_management,
      c( "Early",
         "Recommended",
         "Recommended_plus",
         "Late",
         "Late_plus"
      )
   )

```

Lets inspect the data to determine if we need to transform the response variable

```{r}
hist(dat1$grain_yield.t.ha)
hist(log(dat1$grain_yield.t.ha))
hist(sqrt(dat1$grain_yield.t.ha))
hist(dat1$grain_SMD)
```
Standardised mean differences have a bit of a long tail however this is not bad and transformations with log or sqrt will not work on negative values.  

Lets undertake the meta-analysis, we are using the `spray_management` variable as a moderator and a interactive term to the `trial` variable.
Because we want to know the difference between all treatments and we have no reference treatment, we will remove the intercept.
```{r Metafor-analysis}
PM_mv <- rma.mv(
   yi = grain_SMD,
   vi, 
   mods = ~ spray_management -1,
   method = "ML",
   random = ~ spray_management|trial,
   struct = "UN",
   data = dat1
)
summary(PM_mv)
```
In this result we can see that the `Early` treatment is not significantly different to zero. 
However the other treatments are significantly different to zero.
the Qm omnibus test of moderators show the moderators significantly influence the model and we can reject the null hypothesis that there is no difference between the moderators. 
The Qe test shoes there is a large significant amount of residual Heterogeneity.  

Lets see if we can improve this model by including some other random effects and improving the random effect structure in the model.  


#### CONTINUE FROM HERE!!!!!!!!!!


```{r}

PM_mv_AI2 <- rma.mv(
   yi,
   vi, # vi or yield error
   mods = ~ spray_management,
   method = "ML",
   random = ~ 1|trial,
   data = slimmer_PM_dat
)

anova(PM_mv_AI,PM_mv_AI2) # There is no significant difference between these models, Therefore use the simpler model with the lower AIC (PM_mv_AI2)





PM_mv_AI3 <- rma.mv(
   yi,
   vi, # vi or yield error
   mods = ~ spray_management,
   method = "ML",
   random = list(~ 1 | location/year/trial_ref, ~ 1 | host_genotype, ~ 1 | row_spacing),
   data = slimmer_PM_dat
)

anova(PM_mv_AI2,PM_mv_AI3) # There IS a significant difference between these models, Therefore use the more complicated model with the lower AIC (PM_mv_AI3)



PM_mv_AI4 <- rma.mv(
   yi,
   vi, # vi or yield error
   mods = ~ spray_management,
   method = "ML",
   random = list(~ 1 | location/year/trial_ref, ~ 1 | host_genotype),
   data = slimmer_PM_dat
)

anova(PM_mv_AI3,PM_mv_AI4) # There is no significant difference between these models, Therefore use the simpler model with the lower AIC (PM_mv_AI4)

profile(PM_mv_AI4, sigma2 = 1)

PM_mv_AI5 <- rma.mv(
   yi,
   vi, # vi or yield error
   mods = ~ spray_management,
   method = "ML",
   random = list(~ 1 | location/year/trial_ref),
   data = slimmer_PM_dat
)

anova(PM_mv_AI4,PM_mv_AI5) # There is no significant difference between these models, Therefore use the simpler model with the lower AIC (PM_mv_AI4)

summary(PM_mv_AI4)




PM_mv_AI6 <- rma.mv(
   yi,
   vi, # vi or yield error
   mods = ~ spray_management,
   method = "ML",
   random = list(~ 1 | location/year/trial_ref, ~ 1 | host_genotype, ~ spray_management | D_pres),
   data = slimmer_PM_dat
)

anova(PM_mv_AI6,PM_mv_AI4) # There is no significant difference between these models, Therefore use the simpler model with the lower AIC (PM_mv_AI4)







anova(PM_mv_AI2, PM_mv_AI3)

summary(PM_mv_AI2)

slimmer_PM_dat[is.na(slimmer_PM_dat$AUDPC_m),]
slimmer_PM_dat[slimmer_PM_dat$location == "Emerald", ]
table(slimmer_PM_dat$spray_management)
```
Lets try analysing the data as a risk reduction. 
For this we will use the proportion of yield reductions
```{r}
dat1 <- slimmer_PM_dat[!slimmer_PM_dat$fungicide_ai == "control",]

dat1$yi <- dat1$grain_SMD

PM_mv_MD <- rma.mv(
   yi = yi,
   vi, # vi or yield error
   mods = ~ spray_management - 1,
   method = "ML",
   random = list(~ 1 | location/year , ~1|host_genotype ,~ spray_management|D_pres),
   data = dat1
)

summary(PM_mv_MD)

dat1$D_pres

hist(dat1$grain_SMD)

```




#### Investigate impact of imputing Mean square errors on the meta-analysis outcomes

```{r Initiak_ReRunMetafor, eval=FALSE, include=FALSE}
# It was noticed that the meta analysis was giving different outcomes.
#     (not just small variations in estimates and p-values, but large variations)
# The following chunks investigate how much these values vary and change the outcomes
#
# This chunk creates a data.frame from the meta-analysis which was run in the previous chunk `Metafor-analysis`
# and sets the iteration number as 0
# this data.frame is appended with additional iterations to in the following chunk

summary(PM_mv_AI)

dat1 <- data.frame(
   iteration = 0,
   mods = names(coef(PM_mv_AI)),
   coefs = coef(PM_mv_AI),
   se = PM_mv_AI$se,
   zval = PM_mv_AI$zval,
   pval = PM_mv_AI$pval,
   ci.lb = PM_mv_AI$ci.lb,
   ci.ub = PM_mv_AI$ci.ub,
   row.names = NULL
)

# commented the code below so the file does not get overwritten
#write.csv(dat1, "./cache/metaIteration.csv", row.names = FALSE)
```

```{r run_iterations_asJobscript, eval=FALSE, include=FALSE}
# This chunk iterates the imputation of MSEs and reruns the metafor analysis
# each time it iterates it appends some of the output information into the "metaIteration.csv" file
# To increase the speed the calculations/iterations it is send to multiple jobs in the background.
#
# WARNING don't run a high number of iterations without increasing sys.sleep
# sys.sleep should not be reduced below 4 for 50 iterations and
# below 0.5 for 10 iterations
for (ITERATE in 1:50) {
   rstudioapi::jobRunScript(
      "./R/ReRun_metafor-grainyield.R",
      workingDir = here::here(),
      name = "metafor_iteration",
   )
   Sys.sleep(15)
}
```


```{r coef_iteration, eval=FALSE, include=FALSE}
# This chunk plots the coefficients and corresponding confidence intervals 
#     for each iteration of the metafor calculation to visualise how much variation 
#     the imputed MSE imbue on the moderator coefs

# This function alters the moderator names so they are plotted legibly in the facet strips
change_labels <- function(x1){
   x1 <- gsub(pattern = "spray_management",replacement = "",x = x1)
   x1 <- gsub(pattern = "_",replacement = "\n",x = x1)
   x1
}

metaI <- read.csv("./cache/metaIteration.csv")
metaI %>%
   ggplot(aes(x = iteration ,y = coefs,))+
   geom_ribbon(aes(ymax = ci.ub, ymin = ci.lb, x = iteration), fill = "grey70")+
   geom_line()+
   facet_grid(cols = vars(mods), labeller = labeller(mods = change_labels))+
   theme(strip.text.x = element_text(size = 9)) # change facet strip label font size
   
```

```{r pval_iteration_plot, eval=FALSE, include=FALSE}
# Plot the p - values of the spray_management moderators
# transform y axis on log scale
metaI %>%
   ggplot(aes(x = iteration , y = pval, )) +
   geom_line() +
   scale_y_log10() +
   geom_hline(
      yintercept = c(0.05, 0.001),
      colour = "grey50",
      linetype = 2
   ) +
   facet_grid(cols = vars(mods),
              labeller = labeller(mods = change_labels)) +
   theme(strip.text.x = element_text(size = 9)) +
   geom_text(
      aes(
         y = 0.05,
         x = 1,
         label = c("P = 0.05")
      ),
      size = 3,
      colour = "grey50",
      hjust = 0,
      vjust = -0.5
   )+
      geom_text(
      aes(
         y = 0.001,
         x = 1,
         label = c("P = 0.001")
      ),
      size = 3,
      colour = "grey50",
      hjust = 0,
      vjust = -0.5
   )


```

```{r metafor_summary}
summary(PM_mv_AI)
```

According to the results of the [omnibus test](http://www.metafor-project.org/doku.php/tips:models_with_or_without_intercept?s[]=anova) ($Q_M = 7.32, df = 5, p = 0.1979$) we can't reject the null hypothesis $H_0 : \beta_1 = \beta_2 = \beta_3 =\beta_4 = 0$ that the tested spray management schedules have no benefit to protecting mungbean yields [@Viechtbauer2010]. 
However Late plus ($\beta_4$) was the only moderator showing a significant effect on yields ($z = 2.598; p = 0.0094$). 
The analysis shows there is still a significant amount of residual heterogeneity ($Q_E = 538.26, df = 141, p < 0.0001$) not captured by the spray management moderator indicating other possible moderators which might influence grain yield.  


The first table in this output shows the tau^2 (variance) of each random effects and the number of occurrences for each treatment in the analysis. The second table is in two parts(left and right). The left part indicates the level of acceptable rho (variation) when comparing treatments. All comparisons were acceptable except for a comparison between `Early` and `Late_plus`, `0.000` rho. `Early` and `Late_plus` treatments never occurred within the same trial which is indicated by the right side of the table.  

The fixed effects, in the last table, showed that yields in single early spray treatments are not significantly different to the no spray control. 
Also commencing spray management schedules at first sign of disease (Recommended), or between 7 - 19 days after first sign (late) produced significantly higher yields compared to the no spray control. 
On average a spray schedule with two or more applications stating late (7 - 19 days after first sign of powdery mildew) produced the highest yields.


*****

To make it easier to compare each of the treatments we can compute the meta-analysis contrasts.
```{r metafor_contrasts}
meta_cont <- anova(PM_mv_MD, L = rbind(
   c(0, 1, -1, 0, 0,0),
   # early vs Recommended
   c(0, 1, 0, -1, 0,0),
   # early vs recommended plus
   c(0, 1, 0, 0, -1,0),
   # early vs Late 
   c(0, 1, 0, 0, 0,-1),
   # early vs Late plus
   c(0, 0, -1, 1, 0,0),
   # Recommended plus vs recommended
   c(0, 0, -1, 0, 1,0),
   # Late  vs Recommended
   c(0, 0, -1, 0, 0,1),
   # Late_plus  vs Recommended
   c(0, 0, 0, -1, 1,0),
   # Late vs Recommended plus
   c(0, 0, 0, -1, 0,1),
   # Late_plus  vs Recommended plus
   c(0, 0, 0, 0, -1,1)
)) # Late vs Late_plus



change_labels <- function(x1){
   x1 <- gsub(pattern = "spray_management",replacement = "",x = x1)
   x1
}

data.frame(contrasts = change_labels(meta_cont$hyp[,1]),
      estimates = meta_cont$Lb,
      se = meta_cont$se,
      z_val = meta_cont$zval,
      pval = meta_cont$pval)
```
Results show with the exclusion of the no spray control, none of the treatments are significantly different, however, early applications treatments on average produced lower yields. Also the model was almost significant when assessing `Late_plus` as saving more yield than `Recommended`. This is probably due to the fact there were more comparisons between these two treatments which allowed a more certain outcome.
Let's view these comparisons in a plot.

First we will format the results into a data frame and back-transform the log values by using the exponent function
```{r metafor_results}
results_AI <- data.frame(cbind(exp(PM_mv_AI$b),
                               exp((PM_mv_AI$ci.lb)),
                               exp(PM_mv_AI$ci.ub)))

efficacy <- tbl_df(results_AI)
efficacy$Treatment <-
   factor(
      c(
         "control",
         "Early",
         "Recommended",
         "Recommended_plus",
         "Late",
         "Late_plus"
      )
   )

efficacy$se <- PM_mv_AI$se
colnames(efficacy) <-
   c("Mean", "CIs_lower", "CI_upper", "Treatment", "SE")
efficacy
```

```{r metafor_plot}
efficacy %>%
   ggplot(aes(Treatment, Mean)) +
   geom_hline(
      yintercept = c(0.8, 1, 1.2, 1.4),
      color = "grey80",
      linetype = 3
   ) +
   geom_point(aes(size = 1 / SE), shape = 15) +
   geom_linerange(aes(ymin = CIs_lower, ymax = CI_upper)) +
   coord_flip()
```

Let's look at how well each of the number treatments compare to each other. We can use the netmeta package to give a graphical representation of this.  


### netmeta package  
Let's analyse the data again using a different statistical approach to see if our outcome with the `metafor` package was robust. The `netmeta` package uses a frequentist approach to the analysis and focuses on the pairwise comparisons between treatments.  

```{r netmeta-analysis}
datPM3 <- slimmer_PM_dat %>%
   group_by(trial, spray_management, n) %>%
   summarize(yi_mean = mean(yi),
             vi_mean = mean(vi)) %>%
   ungroup()

PM_con <- pairwise(
   treat = spray_management,
   n = n,
   mean = yi_mean,
   sd = sqrt(vi_mean),
   studlab = trial,
   data = datPM3,
   sm = "MD"
)

net_con <- netmeta(TE,
                   seTE,
                   treat1,
                   treat2,
                   studlab,
                   data = PM_con,
                   sm = "MD")

summary(net_con)
```
Now let's visualise this as a forest plot


```{r netmeta-forest}
# I don't know why this is not working I will need to follow up on this
forest(
   net_con,
   reference.group = 5,
   rightcols = c("effect", "ci", "Pscore"),
   rightlabs = "P-Score",
   small.values = "bad"
)

```

The `netmeta` analysis suggests the spray schedule commencing early are no different to any other treatment including the no spray control. It estimates the mean is very similar to the recommended treatments. The recommended plus and late_plus treatments show higher mean estimates, however not significantly different from the early estimate.  

```{r netgraphGW}
netgraph(
   net_con,
   plastic = FALSE,
   col = "orange",
   thickness =  "number.of.studies",
   points = FALSE,
   col.points = "black",
   cex.points = 1,
   number.of.studies = TRUE,
   cex.number.of.studies = 1,
   col.number.of.studies = "black",
   bg.number.of.studies = "orange",
   multiarm = FALSE,
   col.multiarm = "lightblue",
   pos.number.of.studies = 0.5
)
```


```{r}
netleague(net_con)

decomp.design(net_con)

netsplit(net_con)

nm1 <- netmeasures(net_con)

plot(
   nm1$meanpath,
   nm1$minpar,
   pch = "",
   xlab = "Mean path length",
   ylab = "Minimal parallelism"
)
text(nm1$meanpath, nm1$minpar, names(nm1$meanpath), cex = 0.8)
```



```{r Save_meta_data, eval=FALSE}
write.csv(slimmer_PM_dat, file = "data/GYmeta_data.csv")
```
