---
title: "Meta-analysis"
author: "P. Melloy"
date: "`r Sys.Date()`"
output: html_document
---


```{r MEsummary_Libraries, message=FALSE}
# install.packages("tidyverse")
# install.packages("devtools")
# devtools::install_github("https://github.com/PaulMelloy/bomrang")
# devtools::install_github("https://github.com/adamhsparks/theme.usq")
# install.packages("lme4")


library(RColorBrewer)
library(tidyverse)
library(theme.usq)
library(bomrang)

```

# Meta-analysis

```{r data_import, message=FALSE, echo=FALSE}
source("R/import_data.R")
PM_MB_means <- import_data()

# see Yield vs in-season rain section on the influence of rain and irrigation on grain yield
PM_MB_means$irrigation <- NA
PM_MB_means[PM_MB_means$location == "Hermitage", "irrigation"] <- TRUE
PM_MB_means[PM_MB_means$location != "Hermitage", "irrigation"] <- FALSE

```

## Standardising sample variance  

The type of variance needs to be standardised between trials and treatments for the meta-analysis. 

```{r variance}
# PM_MB_means$Y_error_type <- forcats::fct_explicit_na(PM_MB_means$Y_error_type, na_level = NA)
# Trials using tebuconazole
PM_MB_means %>%
   filter(fungicide_ai == "tebuconazole",
         row_spacing == 0.75) %>%
   group_by(trial_ref, location, Y_error_type) %>%
   summarise(tebuconazole_trts = length(Y_error_type))


# Trials using propiconazole
PM_MB_means %>%
   filter(fungicide_ai == "propiconazole",
          row_spacing == 0.75) %>%
   group_by(trial_ref, location, Y_error_type) %>%
   summarise(propiconazole_trts = length(Y_error_type))
```


### Converting least-squares to sample variance

```{r all_LSD_trials}
Trials <- PM_MB_means %>%
   filter(
      #fungicide_ai == "tebuconazole",
      #    row_spacing == 0.75,
          Y_error_type == "lsd (P=0.05)") %>%
   select(trial_ref) %>%
   distinct()
Trials
```


The trial, `mung1112/02` uses Least Square Differences (LSD) to describe the variation within the experiment.
The following code attempts to convert this to sampling variance as per the method in [Nugugi et.al (2011)](https://apsjournals.apsnet.org/doi/10.1094/PHYTO-08-10-0221).
Ideas for using a T-critical value of 1.697 came from reading a [statisics-how-to website](https://www.statisticshowto.datasciencecentral.com/how-to-calculate-the-least-significant-difference-lsd/).
A [table of T-critical values](https://www.statisticshowto.datasciencecentral.com/tables/t-distribution-table/) was consulted where 30 degrees of freedom within experiment was used to find the 'T-crit' value of 1.697.

![Calculating sampling variance from mean squared error Paul et al. (2008)](Paul_etal_2008.PNG)

```{r LSD_2_Sampling Variance}
# Formula and modifications based off information found at https://www.statisticshowto.datasciencecentral.com/how-to-calculate-the-least-significant-difference-lsd/

for (Trial2012 in Trials$trial_ref) {
   Tcrit <-
      1.697  # 0.05 and DFw = 30 # T critical value for which there is significant difference between two groups; This value is the same for both trials "mung1112/01" and "mung1112/02"
   DFw <-
      sum(PM_MB_means[PM_MB_means$trial_ref == Trial2012, ]$replicates) -
      length(PM_MB_means[PM_MB_means$trial_ref == Trial2012, ]$replicates) # degrees of freedom within groups
   # n.A <- PM_MB_means[PM_MB_means$trial_ref == "Trial2012,]$replicates[1]
   # n.B <- PM_MB_means[PM_MB_means$trial_ref == "Trial2012,]$replicates[2]
   LSD <-
      PM_MB_means[PM_MB_means$trial_ref == Trial2012, ]$yield_error[1]
   
   
   # SSwithin <- #unknown
   # MSW <- SSwithin / DFw # mean square within (SS within / DF within)
   
   
   #  LSD = (Tcrit) * sqrt(MSW * (1/n.A + 1/n.B))
   #  (LSD/Tcrit) = sqrt(MSW * (1/n.A + 1/n.B))
   #  (LSD/Tcrit)^2 = MSW * (1/n.A + 1/n.B)
   #  (LSD/Tcrit)^2/(1/n.A + 1/n.B) = MSW
   #  MSE <- (LSD/Tcrit)^2/(1/n.A + 1/n.B) # This formula was abandon in favor for the formula in Nugugi et. at (2011)
   MSE <-
      PM_MB_means[PM_MB_means$trial_ref == Trial2012, ]$replicates[1] * (LSD /
                                                                            Tcrit) ^ 2 / 2 # method for finding variance in Nugugi et.al (2011) DOI: 10.1094/phyto-08-10-0221
   
   
   for (j in seq_along(PM_MB_means[PM_MB_means$trial_ref == Trial2012, ]$replicates)) {
      if (j == 1) {
         Sv <-
            vector(length = length(PM_MB_means[PM_MB_means$trial_ref == Trial2012, ]$replicates))
      }
      #  Sv[j]^2 = MSE/(PM_MB_means[PM_MB_means$trial_ref == Trial2012,]$replicates[j] *
      #                   PM_MB_means[PM_MB_means$trial_ref == Trial2012,]$grain_yield.t.ha[j])
      # Balance equation
      Sv[j] = sqrt(MSE / (
         PM_MB_means[PM_MB_means$trial_ref == Trial2012, ]$replicates[j] *
            as.numeric(PM_MB_means[PM_MB_means$trial_ref == Trial2012, ]$grain_yield.t.ha[j])
      ))
      
   }
   
   # and replacing the values in the dataset
   
   PM_MB_means[PM_MB_means$trial_ref == Trial2012, ]$yield_error <-
      Sv
   PM_MB_means[PM_MB_means$trial_ref == Trial2012, ]$Y_error_type <-
      "SamplingVariance"
   
}
```


### Converting standard deviation to sample variance
Now to estimate the variance from the standard deviation

```{r stdev2SampVariance}
# subset data by those which describe the Yield error with standard deviation
Trials <- PM_MB_means %>%
   filter(#fungicide_ai == "tebuconazole",
      #row_spacing.m == 0.75,
      Y_error_type == "stdev") %>%
   select(trial_ref) %>%
   distinct()


# calculate the Sampling variance from the standard deviation
# standard error of the mean equals the standard deviation divided by the square root of the number of samples
# Sv <- sem^2 <- stdev/sqrt(n)
# Sampling variance equals the square of the standard error.
# Sv <- (stdev/sqrt(n))^2

# Therefore

for (j in seq_along(PM_MB_means[PM_MB_means$trial_ref %in% Trials$trial_ref,]$yield_error)) {
   # create empty vector at first iteration with length of j
   if (j == 1) {
      Sv <-
         vector(length = length(PM_MB_means[PM_MB_means$trial_ref %in% Trials$trial_ref,]$yield_error))
   }
   
   # Equation to convert standard deviation to Sampling Variance
   Sv[j] = (PM_MB_means[PM_MB_means$trial_ref %in% Trials$trial_ref,]$yield_error[j]) ^
      2
}

# and replacing the values in the dataset
PM_MB_means[PM_MB_means$trial_ref %in% Trials$trial_ref,]$yield_error <-
   Sv
PM_MB_means[PM_MB_means$trial_ref %in% Trials$trial_ref,]$Y_error_type <-
   "SamplingVariance"
```


### Estimating variance for trials without reported error

Now for the trickier part creating 'Sampling Variance' for treatments where only mean yield is recorded.  
The trial which does not report any type of variance also reports that there is no significant difference between the means.

```{r}
NATrials <- PM_MB_means %>%
   filter(fungicide_ai == "tebuconazole",
          row_spacing == 0.75,
          is.na(Y_error_type)) %>%
   select(trial_ref) %>%
   distinct()
```

A single trial `mung1516/03` within the current scope of the meta-analysis fails to report a variance statistic. According to [Machado F. et. al (2017)](http://apsjournals.apsnet.org/doi/10.1094/PDIS-03-17-0340-RE) [Paul et al. (2007)](https://apsjournals.apsnet.org/doi/pdfplus/10.1094/PHYTO-97-2-0211) uses linear regression model to estimate sampling variance.  

#### Mixed effect model to estimate variance

```{r na2Sampling_variance, message=FALSE}
library(lme4)
library(scales)

# for(i in PM_MB_means$trial_ref){
#
# log(V)[i] = a + b * log(
#    mean(subset(PM_MB_means$grain_yield.t.ha[i], fungicide_ai == "control"),
#         subset(PM_MB_means$grain_yield.t.ha[i], fungicide_ai == "tebuconazole")))
# # need to know what parameters a and b are here
# # I am guessing they represent prediction parameters for each "study" for each of the treatments, "control" and tebuconazole"
# }

# method as described by Emerson
PM_MB_train <- PM_MB_means %>%
   filter(Y_error_type == "SamplingVariance")

PM_MB_train$trial_ref <- as.factor(PM_MB_train$trial_ref)

plot(PM_MB_train$yield_error,
     log(PM_MB_train$grain_yield.t.ha))

length(unique(PM_MB_train$trial_ref))

# Run a linear model with grain_yield.t.ha as a fixed effect predictor and trial as a random intercept
PM_MB_trainLMER <- lmer(yield_error ~ grain_yield.t.ha. +
                           (1 | trial_ref), data = PM_MB_train)
summary(PM_MB_trainLMER)

# grain yield (in this model) IS a significant predictor for yield error
# (t value = 5.826)
# And the variance increases as the mean increases does log transformation
# improve the model?


PM_MB_trainLMER.1 <- lmer(log(yield_error) ~ `grain_yield.t.ha` +
                             (1 | trial_ref), data = PM_MB_train)
summary(PM_MB_trainLMER.1) # grain yield a significant predictor of log(yield error)
anova(PM_MB_trainLMER, PM_MB_trainLMER.1) # no significant difference between the models


PM_MB_trainLMER.2 <- lmer(yield_error ~ log(`grain_yield.t.ha`) +
                             (1 | trial_ref), data = PM_MB_train)
summary(PM_MB_trainLMER.2) # log(grain yeild) is a significant predictor of yield error
anova(PM_MB_trainLMER, PM_MB_trainLMER.2) # no significant difference between the models


PM_MB_trainLMER.3 <-
   lmer(log(yield_error) ~ log(`grain_yield.t.ha`) +
           (1 | trial_ref),
        data = PM_MB_train)
summary(PM_MB_trainLMER.3) # log(grain yeild) is a significant predictor of log(yielderror)
anova(PM_MB_trainLMER, PM_MB_trainLMER.3) # no significant difference between the models


# Lets try with Fungicide as a fixed effect
PM_MB_trainLMER.4 <-
   lmer(yield_error ~ `grain_yield.t.ha` + as.factor(fungicide_ai) +
           (1 | trial_ref),
        data = PM_MB_train)
summary(PM_MB_trainLMER.4)
anova(PM_MB_trainLMER, PM_MB_trainLMER.4) # no significant difference between the models



#log transforming the response or predictor variables did not improve the model therefore I will leave it untransformed
# I will be using the first model PM_MB_trainLMER for further analyses beyond this point

# Extract the model coefficients
paramA <-
   (summary(PM_MB_trainLMER)$coef)[2, 1] # Extract the overall mean slope to predict Yield error
InterC <-
   (summary(PM_MB_trainLMER)$coef)[1, 1] # Extract the overall mean intercept to predict Yield error
lmer.coef <-
   coef(PM_MB_trainLMER) # extracting the mean random intercept for each of trial variables






# for(i in NATrials$Trial_ref){
# #Sv <- (log(PM_MB_means[PM_MB_means$Trial_ref == i,]$`grain_yield.t.ha.`) - InterC)/ paramA  # parameter from when I had the predictor and response variable flipped
# 
# Sv <- (PM_MB_means[PM_MB_means$Trial_ref == i,]$`grain_yield.t.ha.` * paramA) + InterC
# 
# PM_MB_means[PM_MB_means$Trial_ref == i,]$Yield_error <- Sv
# PM_MB_means[PM_MB_means$Trial_ref == i,]$Y_error.type <- "SamplingVariance"
# }

# plot of the mixed effect model, Black line shows the mean lmer regression line
ggplot(PM_MB_train) +
   geom_point(aes(y = yield_error, x = grain_yield.t.ha, colour = trial_ref)) +
   geom_abline(
      intercept = c(InterC, lmer.coef$trial_ref[, 1]),
      slope = c(paramA, lmer.coef$trial_ref[, 2]),
      size = 1,
      colour = c("Black", hue_pal()(length(unique(PM_MB_train$trial_ref))))
   ) +
   coord_cartesian(xlim = c(0, 3)) +
   theme_usq()
```



```{r na2Sv_formula}
# commented until a reliable model can be written


# for (i in NATrials$trial_ref) {
#    #Sv <- (log(PM_MB_means[PM_MB_means$trial_ref == i,]$`grain_yield.t.ha`) - InterC)/ paramA  # parameter from when I had the predictor and response variable flipped
#    
#    Sv <-
#       (PM_MB_means[PM_MB_means$trial_ref == i,]$`grain_yield.t.ha` * paramA) + InterC
#    
#    PM_MB_means[PM_MB_means$trial_ref == i,]$yield_error <- Sv
#    PM_MB_means[PM_MB_means$trial_ref == i,]$Y_error_type <-
#       "SamplingVariance"
# }

```


Let's look at the model without the trial mung1718/01, as it looks to be adding some skew.

```{r lmer_remove_mung1718-01}
# remove from mung1718/01 from training dataset
PM_MB_train.2 <-
   PM_MB_train[PM_MB_train$trial_ref != "mung1718/01",]

# Rerun the model
PM_MB_trainLMER.5 <- lmer(yield_error ~ `grain_yield.t.ha` +
                             (1 | trial_ref), data = PM_MB_train.2)
summary(PM_MB_trainLMER.5) # no significant effect of mean grain yield on yield error


# Extract the cooeficients
paramA <-
   (summary(PM_MB_trainLMER.5)$coef)[2, 1] # Extract the overall mean Slope to predict Yield error
InterC <-
   (summary(PM_MB_trainLMER.5)$coef)[1, 1] # Extract the overall mean intercept to predict Yield error
lmer.coef <-
   coef(PM_MB_trainLMER.5) # extracting the mean random intercept for each of trial variables


# plot the data with the predicted slopes overlayed

ggplot(PM_MB_train.2) +
   geom_point(aes(y = yield_error, x = grain_yield.t.ha, colour = trial_ref)) +
   geom_abline(
      intercept = c(InterC, lmer.coef$trial_ref[, 1]),
      slope = c(paramA, lmer.coef$trial_ref[, 2]),
      size = 1,
      colour = c("Black", hue_pal()(length(unique(PM_MB_train.2$Trial_ref))))
   ) +
   coord_cartesian(ylim = c(0, 0.33)) +
   xlab("Grain Yield (t/ha)") +
   ylab("Yield Error") +
   scale_colour_discrete("Trial Reference") +
   theme_usq()

```


#### Random effects model to predict trial variance
Just for kicks lets look at the linear regression model as a random effects model where `grain_yield.t.ha` is set as a random slope to the random intercept `trial_ref`.

The model fits better as shown by an ANOVA comparison. But how to impute from this model if mean variance for a trial is unknown...

```{r random_slope_model} 
# This code chunk broke with the addition of new data, I will re-inspect the data and remodel.

# Run a linear model with yield error as a fixed effect predictor and trial as a random intercept
PM_MB_trainLMERx <- lmer(yield_error ~ 1 +
                            (grain_yield.t.ha |
                                trial_ref), data = PM_MB_train)


anova(PM_MB_trainLMER, PM_MB_trainLMERx) # random slope model NOT significantly better # P = 0.6677

summary(PM_MB_trainLMERx)

InterC <-
   (summary(PM_MB_trainLMER)$coef)[1, 1] # Extract the overall mean intercept

lmer.coefx <- coef(PM_MB_trainLMERx)

# plot of the mixed effect model, Black line shows the mean lmer regression line
ggplot(PM_MB_train) +
   geom_point(aes(y = yield_error, x = grain_yield.t.ha, colour = trial_ref)) +
   geom_abline(
      intercept = lmer.coefx$trial_ref[, 2],
      slope = lmer.coefx$trial_ref[, 1],
      size = 1,
      colour = hue_pal()(length(unique(PM_MB_train$Trial_ref)))
   ) +
   coord_cartesian(xlim = c(0, 3),
                   ylim = c(0, 1)) +
   xlab("Grain Yield (t/ha)") +
   ylab("Yield Error") +
   scale_colour_discrete("Trial Reference") +
   theme_usq()
```
After much deliberation I don't think this is possible to use the model with grain yield t/ha as a random slope. Even though we know the grain yields and the mean of the grain yields, we still can't predict the effect of the random intercept trial on the yield error.


I also re-ran the model without including the trial in which we used LSD to interpret the sampling variance and the model lost a lot of significance.  



## Between trial variation

Emerson proposed I look at the between trial variation
```{r between_trial_variation}
unique(PM_MB_means$Y_error_type)

PM_MB_means.SV <- PM_MB_means[PM_MB_means$Y_error_type == "SamplingVariance",]
meta_mean.GY <- mean(PM_MB_means.SV$grain_yield.t.ha.,na.rm = TRUE)


# find the mean and variance of each trial
PM_MB_means_T <- PM_MB_means %>%
   filter(Y_error_type == "SamplingVariance") %>%
   group_by(trial_ref) %>%
   summarise(trial_mean = mean(grain_yield.t.ha., na.rm = TRUE),
             BW_trial_variance = sd(grain_yield.t.ha., na.rm = TRUE) ^ 2
             ) # I am not sure I have calculated between trial variance or trial variance// Should I be using a denominator (N-1) or (N), sd by default uses (N-1)

# by excluding the standard deviation of each treatment I have discarded much of the variance. Should I simulate a larger number of samples for each treatment to reflect the sample distribution of each treatment and then take the mean and standard deviation of the whole trial of the simulated data?



# update the main data set with Trial variance
SV_Trials <- unique(PM_MB_means_T$Trial_ref)
PM_MB_means$TrialVariance <- NA

for(i in SV_Trials){
   PM_MB_means[PM_MB_means$Trial_ref == i, "TrialVariance"] <- PM_MB_means_T[PM_MB_means_T$Trial_ref == i,"BW_trial_variance"]
}
PM_MB_means$TrialVariance
# OR ?
PM_MB_means <- left_join(PM_MB_means, PM_MB_means_T, by = "trial_ref")

SV_mod1 <- lm(Trial_variance ~ trial_mean, data = PM_MB_means_T)
summary(SV_mod1)

plot(Trial_variance ~ trial_mean, data = PM_MB_means_T)
abline(SV_mod1)


```
